{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0479e25c001874fda15f875d76f8663885caa138"
   },
   "source": [
    "***최초 작성일 : 22.10.5***<br>\n",
    "\n",
    "***최종 작성일 : 22.10.18***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"이유한님의 캐글 코리아 캐글 스터디 커널 커리큘럼\"에 따라 필사한 내용입니다.\n",
    "\n",
    "- 필사 노트북의 원 출처 : https://www.kaggle.com/code/skooch/xgboost/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0479e25c001874fda15f875d76f8663885caa138"
   },
   "source": [
    "## LGMB with random split for early stopping\n",
    "\n",
    "**Edits by Eric Antoine Scuccimarra** <br>\n",
    "이 노트는 Misha Losvyi가 작성한 https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro 의 포크이며 몇 가지 변동사항이 있는데 아래와 같다.\n",
    " - 기존의 LightGBM 모델을 XGBoost 모델로 변경하였다.\n",
    " - VotingClassifiers을 RandomForest로 기본 설정하고 RFs과 XGBs의 결과를 앙상블 하였다.\n",
    " - 일부 feature가 추가되었다.\n",
    " - 이전에 삭제했던 feature를 삭제하지 않고 사용하였다.\n",
    " - 일부 코드를 재구성하였다.\n",
    " - 데이터를 쪼개서 검증 데이터로 early stopping 하는 것이 아닌, 학습 중에 데이터가 쪼개서 학습 데이터 전체가 훈련되도록 하였다. 이 방법은 k-fold 보다 더 좋은 결과를 얻은 것을 발견했다.\n",
    " \n",
    "몇 가지 추가한 feature들은 다음 출처에서 가져왔다.: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman.\n",
    " \n",
    "**원본 커널의 노트(edited by EAS):**\n",
    "\n",
    "이 커널은 https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro 거의 따르지만, 하이퍼 파라미터가 아닌 이 커널의 최적 값을 사용하여 더 빠르게 실행된다.\n",
    "\n",
    "몇 가지 핵심 사항:\n",
    "- **이 커널은 가장(heads of housholds)을 기준으로 학습한다.** (가구에 대한 정보를 집계한 후). 가장을 기준으로 학습하는 것은 이미 알려진 전략에 따른 것이다: *유의해야할 것은 오직 가구주에 대한 정보만 점수(평가지표에 대한 값)를 알 수 있다는 것이다. 모든 가구원은 test + 샘플 제출에 포함되지만, 가구주만 점수를 받는다.* (데이터 설명에 의해서). 그러나, 현재 평가는 가구주뿐만이 아니라 가구원에도 달려있다. (참고 https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115). 실제로, 전체 예측 결과는 ~0.4 PLB 이며, 가장을 제외한 가구원들에게 1 값을 부여하면 ~0.2 PLB 이다.\n",
    "- **class의 빈도를 맞추는 것은 매우 중요하다.** 클래스의 빈도를 맞추지 않은 모델의 경우 로컬에서 ~0.39 PLB / ~0.43 이며, 클래스의 빈도를 맞춘 경우 로컬에서 ~0.42 PLB / 0.47 이다. 클래스 빈도는 직접 수정할 수도 있고, undersampling을 이용할 수도 있다. 그러나 더 간단하고 좋은 성능을 내는 것은 sklearn에서 LightGBM에 제공되는 `class_weight='balanced'` 이다.\n",
    "- **이 커널은 모델 학습 시 macro F1 score를 기준으로 early stopping 기능을 사용한다**.\n",
    "- 범주형 변수는 blind label encoding 대신에 적절한 숫자로 매핑된다.  # blind label encoding이 뭐지..?\n",
    "- **One-hot encoding을 label encoding으로 바꾸면 트리 모델의 경우 더 쉽게 학습할 수 있다.** 다만 트리 모델이 아닌 경우는 One-hot coding이 낫다.\n",
    "- **idhogar는 학습에 사용하지 않는다**. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n",
    "- **가구 변수 내에서 집계를 내며 새로운 feature는 수작업으로 생성한다**. 집계 함수를 적용할 수 있는 변수들은 정해져 있기 때문.\n",
    "- **VotingClassifier는 Light GBM 모델을 평균 내는데 사용한다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 수정한 부분\n",
    "import sys\n",
    "import joblib\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "#\n",
    "\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e4e08a17549fd247619178c96c3ade2519e9773"
   },
   "source": [
    "아래의 범주형 변수 처리는 [이 커널](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)에서 비롯 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# this only transforms the idhogar field, the other things this function used to do are done elsewhere\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "# plot feature importance for sklearn decision trees    \n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        # Print the feature ranking\n",
    "        print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1785c8ca3467a7e95d007a2c5f36e39919fc0910"
   },
   "source": [
    "**또한 feature engineering 역시 아래와 같이 변한다:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9c9f13e54fc2af9f938b895959631e1aeb3b08a2"
   },
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "               }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "\n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "\n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a7df32a07c9157ee02ff9688cdc70c69e7571aae"
   },
   "outputs": [],
   "source": [
    "# convert one hot encoded fields to label encoding\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        #deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy colmn name to be added\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eab84429fc9893c82e33b8319161c190b4104e9f"
   },
   "source": [
    "# Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e6f696a1677230c565532f141a02852e7c69b2e1"
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/train.csv')\n",
    "# test = pd.read_csv('../input/test.csv')\n",
    "train = pd.read_csv('./dataset/costa-rican-household-poverty-prediction/train.csv')\n",
    "test = pd.read_csv('./dataset/costa-rican-household-poverty-prediction/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "bd1f66cbdbfa4741d19a8b1f53793b967d62d281"
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # encode the idhogar\n",
    "    encode_data(df_)\n",
    "    \n",
    "    # create aggregate features\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aaf8a52a939c7d5bc548cbe4ecc1458caae60e7d"
   },
   "source": [
    "Clean up some missing data and convert objects to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "65dab0e9a94e8f87a7b73e7ec2c6559e4ccef996",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some dependencies are Na, fill those with the square root of the square\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# fill \"no\"s for education with 0s\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "\n",
    "# if education is \"yes\" and person is head of household, fill with escolari\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "# convert to int for our models\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# create feature with max education of either head of household\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "# fill some nas\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n",
    "# if there is no water we'll assume they do not\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b564a6552f0521581af1ee38d6040ef7ae5d2fb5"
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "2695108e103c9c61088fc4c100d01bc8c0f4138c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "# convert the one hot fields into label encoded\n",
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e90e68abd266c9db808dbf336308cef7175886bd"
   },
   "source": [
    "# Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "0ffc288b3829ffdb1dabf8f2e7fe518f2f520480"
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# add some aggregates by geography\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "bc96d527090d9b0723049c5d763c97be5145b8c7"
   },
   "outputs": [],
   "source": [
    "# add the number of people over 18 in each household\n",
    "train['num_over_18'] = 0\n",
    "# train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "# test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "# add some extra features, these were taken from another kernel\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a6ed97e10f021b42a19c7228dfc1a52ce9de2c60"
   },
   "outputs": [],
   "source": [
    "# drop duplicated columns\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female', ]\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6e1ccce811e7a1d76282fcb8a13edf92672f834"
   },
   "source": [
    "## Split the data\n",
    "\n",
    "동일한 가구에 속한 행은 대개 동일한 대상을 가지므로 타겟 누출(target leakage)을 방지하기 위해 데이터를 가구별로 분할했습니다.<br>\n",
    "가장만 포함하도록 데이터를 필터링하기 때문에 기술적으로 필요하지 않지만 전체 교육 데이터 세트를 쉽게 사용할 수 있습니다.\n",
    "\n",
    "데이터를 분할한 후에는 모든 데이터에 대해 학습할 수 있도록 전체 데이터 세트로 학습 데이터를 덮어씁니다.<br>\n",
    "split_data 함수는 데이터를 덮어쓰지 않고 동일한 작업을 수행하며, K-fold split을 근사하기 위해 훈련 루프 내에서 사용된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "da37e677a32477006e8468b954e23d595c82eced"
   },
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
    "    # uncomment for extra randomness\n",
    "#     np.random.seed(seed=seed)\n",
    "    \n",
    "    train2 = train.copy()\n",
    "    \n",
    "    # pick some random households to use for the test data\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    \n",
    "    # select households which are in the random selection\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "e14a9619ca6516b225b55bf65d6a9e423d6b5fc7"
   },
   "outputs": [],
   "source": [
    "# query 함수\n",
    "# - 장점은 가독성과 편의성이 최대 장점\n",
    "# - 단점은 .loc[ ] 로 구현한 것보다 속도가 느림\n",
    "\n",
    "X = train.query('parentesco1==1') # 가장일 경우\n",
    "# X = train.copy()\n",
    "\n",
    "# pull out and drop the target variable\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "# idhogar: 가구의 고유 식별자\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "# train on entire dataset\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "871cd5b46da553f6062cef605acc761cddc2a8c0"
   },
   "outputs": [],
   "source": [
    "# figure out the class weights for training with unbalanced classes\n",
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d9bcfbfcfc0b5b7e3aaeb2b0c8495bd92fdf51a3"
   },
   "outputs": [],
   "source": [
    "# drop some features which aren't used by the LGBM or have very low importance\n",
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "67a0f3b7d019517ed9a40b41df318284a33a5ce1"
   },
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2dc384aeb44db2454978df78fdbb84b2b1ff3ced"
   },
   "source": [
    "# Fit a voting classifier\n",
    "Define a derived VotingClassifier class to be able to pass `fit_params` for early stopping. Vote based on LGBM models with early stopping based on macro F1 and decaying learning rate.\n",
    "\n",
    "The parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "629cf88dd0596a98d58487495e5b096636e2586a"
   },
   "outputs": [],
   "source": [
    "# 4\n",
    "# opt_parameters = {'max_depth':35,\n",
    "#                   'eta':0.1,\n",
    "#                   'silent':0,\n",
    "#                   'objective':'multi:softmax',\n",
    "#                   'min_child_weight': 1,\n",
    "#                   'num_class': 4,\n",
    "#                   'gamma': 2.0,\n",
    "#                   'colsample_bylevel': 0.9,\n",
    "#                   'subsample': 0.84,\n",
    "#                   'colsample_bytree': 0.88,\n",
    "#                   'reg_lambda': 0.40 }\n",
    "# 5\n",
    "opt_parameters = {'max_depth':35,\n",
    "                  'eta':0.15,\n",
    "                  'silent':1,\n",
    "                  'objective':'multi:softmax',\n",
    "                  'min_child_weight': 2,\n",
    "                  'num_class': 4,\n",
    "                  'gamma': 2.5,\n",
    "                  'colsample_bylevel': 1,\n",
    "                  'subsample': 0.95,\n",
    "                  'colsample_bytree': 0.85,\n",
    "                  'reg_lambda': 0.35 }\n",
    "# 6\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# # 7\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    predictions = np.expand_dims(predictions, axis=1)\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":500,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'verbose': False,\n",
    "           }\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "101a2fd45bbbe6c7fb351513803550d4edeef2b3"
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    # randomly split the data so we have a test set for early stopping\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "        \n",
    "    # update the fit params with our new split\n",
    "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
    "    \n",
    "    # fit the estimator\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
    "        print(\"Train F1:\", best_train)\n",
    "        print(\"Test F1:\", best_cv)\n",
    "        \n",
    "    # reject some estimators based on their performance on train and test sets\n",
    "    if threshold:\n",
    "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "\n",
    "        # else recurse until we get a better one\n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    '''\n",
    "    This implements the fit method of the VotingClassifier propagating fit_params\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "37b909c2aa273651b7bb57c69b939760f14f38f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:10:10] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.29680\tvalidation_0-macroF1:0.95622\n",
      "[50]\tvalidation_0-mlogloss:0.90523\tvalidation_0-macroF1:0.95622\n",
      "[100]\tvalidation_0-mlogloss:0.90513\tvalidation_0-macroF1:0.95622\n",
      "[150]\tvalidation_0-mlogloss:0.90356\tvalidation_0-macroF1:0.95622\n",
      "[200]\tvalidation_0-mlogloss:0.90421\tvalidation_0-macroF1:0.95622\n",
      "[250]\tvalidation_0-mlogloss:0.90182\tvalidation_0-macroF1:0.95622\n",
      "[299]\tvalidation_0-mlogloss:0.90283\tvalidation_0-macroF1:0.95622\n",
      "Train F1: 0.7730345420867885\n",
      "Test F1: 0.37652341301817527\n",
      "[18:10:21] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30169\tvalidation_0-macroF1:0.95763\n",
      "[50]\tvalidation_0-mlogloss:0.90627\tvalidation_0-macroF1:0.95763\n",
      "[100]\tvalidation_0-mlogloss:0.90505\tvalidation_0-macroF1:0.95763\n",
      "[150]\tvalidation_0-mlogloss:0.90486\tvalidation_0-macroF1:0.95763\n",
      "[200]\tvalidation_0-mlogloss:0.90528\tvalidation_0-macroF1:0.95763\n",
      "[250]\tvalidation_0-mlogloss:0.90367\tvalidation_0-macroF1:0.95763\n",
      "[299]\tvalidation_0-mlogloss:0.90179\tvalidation_0-macroF1:0.95763\n",
      "Train F1: 0.7492942683863719\n",
      "Test F1: 0.37656015100131635\n",
      "[18:10:34] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30207\tvalidation_0-macroF1:0.97068\n",
      "[50]\tvalidation_0-mlogloss:0.95605\tvalidation_0-macroF1:0.97068\n",
      "[100]\tvalidation_0-mlogloss:0.94580\tvalidation_0-macroF1:0.97068\n",
      "[150]\tvalidation_0-mlogloss:0.94411\tvalidation_0-macroF1:0.97068\n",
      "[200]\tvalidation_0-mlogloss:0.94359\tvalidation_0-macroF1:0.97068\n",
      "[250]\tvalidation_0-mlogloss:0.94204\tvalidation_0-macroF1:0.97068\n",
      "[299]\tvalidation_0-mlogloss:0.94111\tvalidation_0-macroF1:0.97068\n",
      "Train F1: 0.7512947504195298\n",
      "Test F1: 0.36869400159165666\n",
      "[18:10:45] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.31082\tvalidation_0-macroF1:0.96334\n",
      "[50]\tvalidation_0-mlogloss:0.93985\tvalidation_0-macroF1:0.96334\n",
      "[100]\tvalidation_0-mlogloss:0.93325\tvalidation_0-macroF1:0.96334\n",
      "[150]\tvalidation_0-mlogloss:0.93414\tvalidation_0-macroF1:0.96334\n",
      "[200]\tvalidation_0-mlogloss:0.93416\tvalidation_0-macroF1:0.96334\n",
      "[250]\tvalidation_0-mlogloss:0.93476\tvalidation_0-macroF1:0.96334\n",
      "[299]\tvalidation_0-mlogloss:0.93293\tvalidation_0-macroF1:0.96334\n",
      "Train F1: 0.7450309123246115\n",
      "Test F1: 0.37302268947493467\n",
      "[18:10:57] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30740\tvalidation_0-macroF1:0.96190\n",
      "[50]\tvalidation_0-mlogloss:0.92836\tvalidation_0-macroF1:0.96190\n",
      "[100]\tvalidation_0-mlogloss:0.92422\tvalidation_0-macroF1:0.96190\n",
      "[150]\tvalidation_0-mlogloss:0.92427\tvalidation_0-macroF1:0.96190\n",
      "[200]\tvalidation_0-mlogloss:0.92541\tvalidation_0-macroF1:0.96190\n",
      "[250]\tvalidation_0-mlogloss:0.92541\tvalidation_0-macroF1:0.96190\n",
      "[299]\tvalidation_0-mlogloss:0.92483\tvalidation_0-macroF1:0.96190\n",
      "Train F1: 0.7480870222135396\n",
      "Test F1: 0.3605319111127716\n",
      "[18:11:09] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30616\tvalidation_0-macroF1:0.96334\n",
      "[50]\tvalidation_0-mlogloss:0.92704\tvalidation_0-macroF1:0.96334\n",
      "[100]\tvalidation_0-mlogloss:0.92303\tvalidation_0-macroF1:0.96334\n",
      "[150]\tvalidation_0-mlogloss:0.91952\tvalidation_0-macroF1:0.96334\n",
      "[200]\tvalidation_0-mlogloss:0.92093\tvalidation_0-macroF1:0.96334\n",
      "[250]\tvalidation_0-mlogloss:0.91994\tvalidation_0-macroF1:0.96334\n",
      "[299]\tvalidation_0-mlogloss:0.92051\tvalidation_0-macroF1:0.96334\n",
      "Train F1: 0.7690755820913078\n",
      "Test F1: 0.3469216962569085\n",
      "[18:11:20] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30328\tvalidation_0-macroF1:0.97068\n",
      "[50]\tvalidation_0-mlogloss:0.91678\tvalidation_0-macroF1:0.97068\n",
      "[100]\tvalidation_0-mlogloss:0.91822\tvalidation_0-macroF1:0.97068\n",
      "[150]\tvalidation_0-mlogloss:0.91615\tvalidation_0-macroF1:0.97068\n",
      "[200]\tvalidation_0-mlogloss:0.91450\tvalidation_0-macroF1:0.97068\n",
      "[250]\tvalidation_0-mlogloss:0.91373\tvalidation_0-macroF1:0.97068\n",
      "[299]\tvalidation_0-mlogloss:0.91581\tvalidation_0-macroF1:0.97068\n",
      "Train F1: 0.7510037924095103\n",
      "Test F1: 0.33615770436998177\n",
      "[18:11:31] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30164\tvalidation_0-macroF1:0.96625\n",
      "[50]\tvalidation_0-mlogloss:0.93069\tvalidation_0-macroF1:0.96625\n",
      "[100]\tvalidation_0-mlogloss:0.92412\tvalidation_0-macroF1:0.96625\n",
      "[150]\tvalidation_0-mlogloss:0.92316\tvalidation_0-macroF1:0.96625\n",
      "[200]\tvalidation_0-mlogloss:0.92266\tvalidation_0-macroF1:0.96625\n",
      "[250]\tvalidation_0-mlogloss:0.92314\tvalidation_0-macroF1:0.96625\n",
      "[299]\tvalidation_0-mlogloss:0.92298\tvalidation_0-macroF1:0.96625\n",
      "Train F1: 0.7766190357989623\n",
      "Test F1: 0.3583571195049386\n",
      "[18:11:43] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.29409\tvalidation_0-macroF1:0.96552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalidation_0-mlogloss:0.87443\tvalidation_0-macroF1:0.96552\n",
      "[100]\tvalidation_0-mlogloss:0.87090\tvalidation_0-macroF1:0.96552\n",
      "[150]\tvalidation_0-mlogloss:0.86691\tvalidation_0-macroF1:0.96552\n",
      "[200]\tvalidation_0-mlogloss:0.86484\tvalidation_0-macroF1:0.96552\n",
      "[250]\tvalidation_0-mlogloss:0.86555\tvalidation_0-macroF1:0.96552\n",
      "[299]\tvalidation_0-mlogloss:0.86487\tvalidation_0-macroF1:0.96552\n",
      "Train F1: 0.7629506642532279\n",
      "Test F1: 0.37458050675068355\n",
      "[18:11:55] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.29922\tvalidation_0-macroF1:0.96625\n",
      "[50]\tvalidation_0-mlogloss:0.89227\tvalidation_0-macroF1:0.96625\n",
      "[100]\tvalidation_0-mlogloss:0.88672\tvalidation_0-macroF1:0.96625\n",
      "[150]\tvalidation_0-mlogloss:0.88420\tvalidation_0-macroF1:0.96625\n",
      "[200]\tvalidation_0-mlogloss:0.88268\tvalidation_0-macroF1:0.96625\n",
      "[250]\tvalidation_0-mlogloss:0.88002\tvalidation_0-macroF1:0.96625\n",
      "[299]\tvalidation_0-mlogloss:0.87854\tvalidation_0-macroF1:0.96625\n",
      "Train F1: 0.7572935578724358\n",
      "Test F1: 0.3587227606120318\n",
      "[18:12:06] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.29696\tvalidation_0-macroF1:0.96552\n",
      "[50]\tvalidation_0-mlogloss:0.93468\tvalidation_0-macroF1:0.96552\n",
      "[100]\tvalidation_0-mlogloss:0.93412\tvalidation_0-macroF1:0.96552\n",
      "[150]\tvalidation_0-mlogloss:0.93732\tvalidation_0-macroF1:0.96552\n",
      "[200]\tvalidation_0-mlogloss:0.93682\tvalidation_0-macroF1:0.96552\n",
      "[250]\tvalidation_0-mlogloss:0.93680\tvalidation_0-macroF1:0.96552\n",
      "[299]\tvalidation_0-mlogloss:0.93861\tvalidation_0-macroF1:0.96552\n",
      "Train F1: 0.7549326750868662\n",
      "Test F1: 0.3673432435940952\n",
      "[18:12:18] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.29918\tvalidation_0-macroF1:0.96772\n",
      "[50]\tvalidation_0-mlogloss:0.92283\tvalidation_0-macroF1:0.96772\n",
      "[100]\tvalidation_0-mlogloss:0.91854\tvalidation_0-macroF1:0.96772\n",
      "[150]\tvalidation_0-mlogloss:0.91629\tvalidation_0-macroF1:0.96772\n",
      "[200]\tvalidation_0-mlogloss:0.91850\tvalidation_0-macroF1:0.96772\n",
      "[250]\tvalidation_0-mlogloss:0.92010\tvalidation_0-macroF1:0.96772\n",
      "[299]\tvalidation_0-mlogloss:0.91997\tvalidation_0-macroF1:0.96772\n",
      "Train F1: 0.749482461473995\n",
      "Test F1: 0.3422818576748416\n",
      "[18:12:29] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30685\tvalidation_0-macroF1:0.96118\n",
      "[50]\tvalidation_0-mlogloss:0.92373\tvalidation_0-macroF1:0.96118\n",
      "[100]\tvalidation_0-mlogloss:0.91398\tvalidation_0-macroF1:0.96118\n",
      "[150]\tvalidation_0-mlogloss:0.91153\tvalidation_0-macroF1:0.96118\n",
      "[200]\tvalidation_0-mlogloss:0.91033\tvalidation_0-macroF1:0.96118\n",
      "[250]\tvalidation_0-mlogloss:0.90955\tvalidation_0-macroF1:0.96118\n",
      "[299]\tvalidation_0-mlogloss:0.90919\tvalidation_0-macroF1:0.96118\n",
      "Train F1: 0.760805809875252\n",
      "Test F1: 0.3464212317759924\n",
      "[18:12:41] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30529\tvalidation_0-macroF1:0.96772\n",
      "[50]\tvalidation_0-mlogloss:0.92785\tvalidation_0-macroF1:0.96772\n",
      "[100]\tvalidation_0-mlogloss:0.92306\tvalidation_0-macroF1:0.96772\n",
      "[150]\tvalidation_0-mlogloss:0.92130\tvalidation_0-macroF1:0.96772\n",
      "[200]\tvalidation_0-mlogloss:0.92189\tvalidation_0-macroF1:0.96772\n",
      "[250]\tvalidation_0-mlogloss:0.92187\tvalidation_0-macroF1:0.96772\n",
      "[299]\tvalidation_0-mlogloss:0.92001\tvalidation_0-macroF1:0.96772\n",
      "Train F1: 0.7671096284517652\n",
      "Test F1: 0.36033454422881306\n",
      "[18:12:52] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.30088\tvalidation_0-macroF1:0.96262\n",
      "[50]\tvalidation_0-mlogloss:0.87972\tvalidation_0-macroF1:0.96262\n",
      "[100]\tvalidation_0-mlogloss:0.87110\tvalidation_0-macroF1:0.96262\n",
      "[150]\tvalidation_0-mlogloss:0.86958\tvalidation_0-macroF1:0.96262\n",
      "[200]\tvalidation_0-mlogloss:0.86576\tvalidation_0-macroF1:0.96262\n",
      "[250]\tvalidation_0-mlogloss:0.86352\tvalidation_0-macroF1:0.96262\n",
      "[299]\tvalidation_0-mlogloss:0.86353\tvalidation_0-macroF1:0.96262\n",
      "Train F1: 0.7688614901116094\n",
      "Test F1: 0.3507177559280246\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4,\n",
    "                            **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "241d2973497b8eb2e5214f5c8ddb76432576ba35",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.6882\n"
     ]
    }
   ],
   "source": [
    "# params 4 - 400 early stop - 15 estimators - l1 used features - weighted\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "# vc.voting = 'soft'\n",
    "# global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "# vc.voting = 'hard'\n",
    "# global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "# print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "# print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "527b58d849cc1282909f15596cd5441ef4cac93d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil4_COUNT',\n",
       " 'agg18_estadocivil5_COUNT',\n",
       " 'geo_energcocinar_LE_0',\n",
       " 'geo_epared_LE_2'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which features are not used by ANY models\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "71dae26394955fcf940feea1c9ba1a5bcf134ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 59 (0.023711) - agg18_escolari_MAX\n",
      "2. feature 74 (0.018926) - agg18_parentesco2_MEAN\n",
      "3. feature 42 (0.017711) - fe_children_fraction\n",
      "4. feature 60 (0.013090) - agg18_escolari_MEAN\n",
      "5. feature 110 (0.012978) - geo_eviv_LE_2\n",
      "6. feature 101 (0.012587) - geo_meaneduc\n",
      "7. feature 22 (0.012228) - dependency\n",
      "8. feature 7 (0.011664) - r4h2\n",
      "9. feature 85 (0.011568) - edjef\n",
      "10. feature 128 (0.011530) - geo_manual_elec_LE_0\n",
      "11. feature 17 (0.011084) - male\n",
      "12. feature 87 (0.010941) - piso_LE\n",
      "13. feature 3 (0.010935) - hacapo\n",
      "14. feature 105 (0.010789) - geo_hogar_total\n",
      "15. feature 40 (0.010612) - SQBdependency\n",
      "16. feature 107 (0.010595) - geo_overcrowding\n",
      "17. feature 124 (0.010381) - geo_sanitario_LE_1\n",
      "18. feature 23 (0.010320) - edjefe\n",
      "19. feature 109 (0.009992) - geo_eviv_LE_1\n",
      "20. feature 12 (0.009935) - r4t1\n",
      "21. feature 15 (0.009868) - cielorazo\n",
      "22. feature 49 (0.009825) - fe_mobile_density\n",
      "23. feature 94 (0.009786) - etecho_LE\n",
      "24. feature 113 (0.009674) - geo_etecho_LE_2\n",
      "25. feature 95 (0.009669) - eviv_LE\n",
      "26. feature 104 (0.009639) - geo_hogar_adul\n",
      "27. feature 36 (0.009540) - SQBhogar_total\n",
      "28. feature 93 (0.009523) - epared_LE\n",
      "29. feature 11 (0.009487) - r4m3\n",
      "30. feature 92 (0.009405) - elimbasu_LE\n",
      "31. feature 100 (0.009394) - geo_age\n",
      "32. feature 106 (0.009371) - geo_bedrooms\n",
      "33. feature 120 (0.009300) - geo_elimbasu_LE_5\n",
      "34. feature 98 (0.009204) - tipovivi_LE\n",
      "35. feature 27 (0.009105) - overcrowding\n",
      "36. feature 35 (0.009102) - SQBage\n",
      "37. feature 14 (0.008973) - escolari\n",
      "38. feature 127 (0.008867) - geo_sanitario_LE_4\n",
      "39. feature 44 (0.008837) - fe_all_man_fraction\n",
      "40. feature 16 (0.008793) - dis\n",
      "41. feature 31 (0.008787) - area1\n",
      "42. feature 19 (0.008740) - hogar_adul\n",
      "43. feature 46 (0.008631) - fe_human_bed_density\n",
      "44. feature 63 (0.008625) - agg18_estadocivil2_MEAN\n",
      "45. feature 45 (0.008554) - fe_human_density\n",
      "46. feature 71 (0.008548) - agg18_estadocivil6_MEAN\n",
      "47. feature 58 (0.008500) - agg18_escolari_MIN\n",
      "48. feature 102 (0.008452) - geo_dependency\n",
      "49. feature 55 (0.008433) - agg18_age_MIN\n",
      "50. feature 69 (0.008360) - agg18_estadocivil5_MEAN\n",
      "51. feature 13 (0.008337) - r4t2\n",
      "52. feature 51 (0.008328) - fe_mobile_adult_density\n",
      "53. feature 25 (0.008276) - meaneduc\n",
      "54. feature 96 (0.008270) - estadocivil_LE\n",
      "55. feature 97 (0.008236) - lugar_LE\n",
      "56. feature 65 (0.008226) - agg18_estadocivil3_MEAN\n",
      "57. feature 4 (0.008191) - refrig\n",
      "58. feature 33 (0.008176) - age\n",
      "59. feature 111 (0.008173) - geo_etecho_LE_0\n",
      "60. feature 132 (0.008095) - geo_pared_LE_0\n",
      "61. feature 86 (0.008075) - pared_LE\n",
      "62. feature 9 (0.008036) - r4m1\n",
      "63. feature 41 (0.008015) - SQBmeaned\n",
      "64. feature 30 (0.007999) - qmobilephone\n",
      "65. feature 20 (0.007996) - hogar_mayor\n",
      "66. feature 131 (0.007988) - geo_manual_elec_LE_4\n",
      "67. feature 37 (0.007810) - SQBedjefe\n",
      "68. feature 116 (0.007804) - geo_elimbasu_LE_0\n",
      "69. feature 62 (0.007784) - agg18_estadocivil1_COUNT\n",
      "70. feature 144 (0.007764) - rent_to_over_18\n",
      "71. feature 43 (0.007531) - fe_working_man_fraction\n",
      "72. feature 91 (0.007515) - energcocinar_LE\n",
      "73. feature 75 (0.007469) - agg18_parentesco3_MEAN\n",
      "74. feature 125 (0.007418) - geo_sanitario_LE_2\n",
      "75. feature 10 (0.007378) - r4m2\n",
      "76. feature 52 (0.007248) - fe_tablet_adult_density\n",
      "77. feature 24 (0.007243) - edjefa\n",
      "78. feature 122 (0.007206) - geo_energcocinar_LE_3\n",
      "79. feature 57 (0.007185) - agg18_age_MEAN\n",
      "80. feature 136 (0.007162) - bedrooms_to_rooms\n",
      "81. feature 50 (0.007130) - fe_tablet_density\n",
      "82. feature 56 (0.007097) - agg18_age_MAX\n",
      "83. feature 61 (0.007092) - agg18_dis_MEAN\n",
      "84. feature 0 (0.007066) - v2a1\n",
      "85. feature 5 (0.007063) - v18q1\n",
      "86. feature 48 (0.006973) - fe_rent_per_room\n",
      "87. feature 18 (0.006935) - hogar_nin\n",
      "88. feature 137 (0.006924) - rent_to_rooms\n",
      "89. feature 47 (0.006915) - fe_rent_per_person\n",
      "90. feature 90 (0.006911) - sanitario_LE\n",
      "91. feature 2 (0.006893) - rooms\n",
      "92. feature 29 (0.006712) - television\n",
      "93. feature 21 (0.006687) - hogar_total\n",
      "94. feature 39 (0.006627) - SQBovercrowding\n",
      "95. feature 26 (0.006611) - bedrooms\n",
      "96. feature 72 (0.006434) - agg18_estadocivil7_MEAN\n",
      "97. feature 67 (0.006325) - agg18_estadocivil4_MEAN\n",
      "98. feature 143 (0.006262) - rent_to_hhsize\n",
      "99. feature 6 (0.006122) - r4h1\n",
      "100. feature 8 (0.006087) - r4h3\n",
      "101. feature 140 (0.006067) - r4t3_to_rooms\n",
      "102. feature 138 (0.006038) - tamhog_to_rooms\n",
      "103. feature 1 (0.005959) - hacdor\n",
      "104. feature 32 (0.005920) - area2\n",
      "105. feature 64 (0.005745) - agg18_estadocivil2_COUNT\n",
      "106. feature 34 (0.005741) - SQBescolari\n",
      "107. feature 79 (0.005614) - agg18_parentesco7_MEAN\n",
      "108. feature 99 (0.005506) - manual_elec_LE\n",
      "109. feature 88 (0.005273) - techo_LE\n",
      "110. feature 114 (0.004550) - geo_epared_LE_1\n",
      "111. feature 141 (0.004441) - v2a1_to_r4t3\n",
      "112. feature 142 (0.004345) - hhsize_to_rooms\n",
      "113. feature 129 (0.004221) - geo_manual_elec_LE_1\n",
      "114. feature 28 (0.004212) - computer\n",
      "115. feature 53 (0.003884) - fe_people_not_living\n",
      "116. feature 83 (0.003859) - agg18_parentesco11_MEAN\n",
      "117. feature 38 (0.003843) - SQBhogar_nin\n",
      "118. feature 81 (0.003836) - agg18_parentesco9_MEAN\n",
      "119. feature 78 (0.003147) - agg18_parentesco6_MEAN\n",
      "120. feature 89 (0.003050) - abastagua_LE\n",
      "121. feature 77 (0.003035) - agg18_parentesco5_MEAN\n",
      "122. feature 84 (0.002585) - agg18_parentesco12_MEAN\n",
      "123. feature 139 (0.001873) - r4t3_to_tamhog\n",
      "124. feature 80 (0.001478) - agg18_parentesco8_MEAN\n",
      "125. feature 76 (0.001406) - agg18_parentesco4_MEAN\n",
      "126. feature 134 (0.000000) - geo_pared_LE_2\n",
      "127. feature 54 (0.000000) - fe_people_weird_stat\n",
      "128. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n",
      "129. feature 73 (0.000000) - agg18_parentesco1_MEAN\n",
      "130. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n",
      "131. feature 135 (0.000000) - geo_pared_LE_7\n",
      "132. feature 123 (0.000000) - geo_sanitario_LE_0\n",
      "133. feature 133 (0.000000) - geo_pared_LE_1\n",
      "134. feature 130 (0.000000) - geo_manual_elec_LE_3\n",
      "135. feature 126 (0.000000) - geo_sanitario_LE_3\n",
      "136. feature 121 (0.000000) - geo_energcocinar_LE_0\n",
      "137. feature 119 (0.000000) - geo_elimbasu_LE_3\n",
      "138. feature 118 (0.000000) - geo_elimbasu_LE_2\n",
      "139. feature 117 (0.000000) - geo_elimbasu_LE_1\n",
      "140. feature 115 (0.000000) - geo_epared_LE_2\n",
      "141. feature 82 (0.000000) - agg18_parentesco10_MEAN\n",
      "142. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n",
      "143. feature 108 (0.000000) - geo_eviv_LE_0\n",
      "144. feature 103 (0.000000) - geo_hogar_nin\n",
      "145. feature 112 (0.000000) - geo_etecho_LE_1\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e78eb892f60a52240aee2b15beaa9f9dfd59994"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "8a98ed5c6da8cd601a4ff2842c4440c16dcca8b8"
   },
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "aa255a424ccbd3839015c82777963dc6b79d8cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9053207939339767\n",
      "Test F1: 0.402407560903425\n",
      "Train F1: 0.8954991388737441\n",
      "Test F1: 0.4070675349127375\n",
      "Train F1: 0.8924444260746862\n",
      "Test F1: 0.37326698439717254\n",
      "Train F1: 0.8899710216055792\n",
      "Test F1: 0.4218502588727234\n",
      "Train F1: 0.894282576194643\n",
      "Test F1: 0.45350659973135826\n",
      "Train F1: 0.8909338335881182\n",
      "Test F1: 0.44117312152247723\n",
      "Train F1: 0.8903438532108738\n",
      "Test F1: 0.41843516547029647\n",
      "Train F1: 0.89636754330742\n",
      "Test F1: 0.45889637464181077\n",
      "Train F1: 0.9068326547636192\n",
      "Test F1: 0.44012255841225995\n",
      "Train F1: 0.8983594585273653\n",
      "Test F1: 0.40719735683694824\n"
     ]
    }
   ],
   "source": [
    "# do the same thing for some extra trees classifiers\n",
    "ets = []    \n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
    "    ets.append(('rf{}'.format(i), rf))   \n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "9eee3dd6964fa71d571aa342c83017b40e6b573e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8609\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "# w/ threshold, extra drop cols\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "86568853e4dd624fc907c312b3786d2193af2ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8609\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "# w/o threshold, extra drop cols\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "1a85b976aeda6675152ec31c2576fef487716ff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_over_18', 'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which features are not used by ANY models\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "33e3f7da04fc8d6099b94885bb6a6e259e56a919"
   },
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5, 0.5]):\n",
    "    # do soft voting with both classifiers\n",
    "#     vc.voting=\"soft\"\n",
    "#     vc1_probs = vc.predict(data.drop(xgb_drop_cols, axis=1))\n",
    "    vc2.voting =\"soft\"\n",
    "    vc2_probs  = vc2.predict(data.drop(et_drop_cols, axis=1))\n",
    "    vc2.voting =\"hard\"\n",
    "    vc2_probs_ = vc2.predict(data.drop(et_drop_cols, axis=1))\n",
    "    \n",
    "#     final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    final_vote = (vc2_probs_ * weights[0]) + (vc2_probs * weights[1])\n",
    "    final_vote = np.expand_dims(final_vote, axis=1)\n",
    "\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "5cd61449e198aca7bef88120dda16b736f1fca7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026595744680851064"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "324c4cf008dc0dadd6c74f2bf4ed0a77e0314c2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026595744680851064"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
    "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "070fc3eeb109d98a71dfe9f35b217ae8646935ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026595744680851064"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78749eec7f69bcc8c587278a2c1a43ac8b5832e3"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32bfd69fe130005cb88865399c460ac00c7b1574"
   },
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "718054937aa849c23ca7c65483f8a52de6f6fd49"
   },
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep=True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep=True)\n",
    "y_subm_ens['Target'] = combine_voters(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "8258f28127f235e427a25f6824566a23df61b8af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
    "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
    "y_subm_ens.to_csv(sub_file_ens, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c6d7f945dec95a777b4221c5fe217c3eea24100",
    "collapsed": true
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터셋 설명 (Costa Rican Household Poverty)\n",
    "\n",
    "미주 개발 은행(Inter-American Development Bank)의 세계에서 가장 빈곤 한 일부 가정의 소득 자격을 예측을 하는 것\n",
    "\n",
    "train set에는 9557개의 row와 143개의 column이 있으며<br>\n",
    "test set에는 23856개의 row와 142개의 column이 있다.\n",
    "\n",
    "각 row는 개인 또는 대한 자산 특징을 나타내며 Target은 개인의 빈곤 수준을 1~4의 값으로 나타낸다. 여기서 1이 가장 극심한 빈곤 수준이다.\n",
    "\n",
    "Id - 각 행(개인)의 고유식별자<br>\n",
    "Target - 개인의 빈곤 수준 (1: 극빈, 2: 적당한 빈곤, 3: 취약계층 가구, 4: 비취약 가구)<br>\n",
    "idhogar - 각 가구에 대한 고유 식별자<br>\n",
    "parentesco1 - 이 사람이 가장인지에 대한 여부<br>\n",
    "\n",
    "이외의 자산 특징을 나타내는 139개의 column들이 있다.\n",
    "\n",
    "출처 - https://foreverhappiness.tistory.com/m/119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 분석 기법 등 이번 노트에서 쓰인 방법론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "앙상블에는 크게 Voting, Bagging, Boosting, Stacking. 총 4가지가 있다.\n",
    "\n",
    "\n",
    "### Voting 방법\n",
    "\n",
    "Voting은 서로 다른 모델들을 데이터 셋으로 학습시키고 각 모델들의 예측값을 평균내거나 최빈값을 계산하는 방식으로 최종 예측값을 계산\n",
    "\n",
    "#### 1. hard voting\n",
    "hard voting은 결과를 예측하여 분류기 중 가장 많은 선택(표)를 받은 결과를 선택하는 것<br>\n",
    "예시\n",
    "<img src=\"image/hard_voting.png\" style=\"width: 400px;\"/><br>\n",
    "\n",
    "#### 2. soft voting\n",
    "soft voting class별로 모델들이 예측한 확률을 합산해서 가장 높은 class를 선택<br>\n",
    "예시\n",
    "<img src=\"image/soft_voting.png\" style=\"width: 400px;\"/><br>\n",
    "\n",
    "출처 - https://devkor.tistory.com/m/entry/Soft-Voting-%EA%B3%BC-Hard-Voting<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 공부한 내용\n",
    " → \n",
    "\n",
    "#### 2. 어려웠던 부분\n",
    " → 어려웠다기보단 xgboost 버전 '1.6.2' 기준으로 해당 커널의 일부 코드가 작동하지 않았다.<br>\n",
    "  따라서 버전을 내렸지만, 맥 환경에서는 python 3.8 버전 이상이 고정이다 보니, xgboost를 1.3 버전 대에 가면 커널 오류가 발생한다.<br>\n",
    "  (1.4 버전 대에서는 해당 노트의 코드 에러가 난다.)<br>\n",
    " \n",
    "#### 3. 느낀점 및 좋던 부분\n",
    " → \n",
    " \n",
    "#### 4. 라이브러리 사용법\n",
    " → \n",
    " \n",
    "#### 5. 해당 커널의 특징(다른 커널과의 차별점)\n",
    " → RF 와 XGB를 각각 VotingClassifier를 사용하여 결과를 냈고 마지막엔 class weight를 0.4~ 0.6씩 주고 weighted average 했다.\n",
    " \n",
    "#### 6. 추후 공부 및 정리할 내용\n",
    " → "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
